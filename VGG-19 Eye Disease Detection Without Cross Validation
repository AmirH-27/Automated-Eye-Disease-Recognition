{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8fs1lduDvaz"
      },
      "source": [
        "# Intel Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPsh3hIkG6VU",
        "outputId": "00c37b85-b759-49af-d5b6-ee689fae2b8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUnVPq-rDva2",
        "outputId": "a377829a-2179-4d50-d8ab-d61e640f16ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "\n",
        "from random import randint\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY8qgOqVDva4"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "IMAGE_SIZE = (224,224)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQigWzppDva5"
      },
      "source": [
        "# Get device name (GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGNbC8dRDva5",
        "outputId": "585359e8-7831-4541-ce5a-18870f53f2d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if \"GPU\" not in device_name:\n",
        "    print(\"GPU device not found\")\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEK8HzyxDva6"
      },
      "source": [
        "# Let's see the available folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUZLbyI4Ef6_",
        "outputId": "ade638bf-f43d-48a1-9207-367beedad38a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMMlw-bcDva6",
        "outputId": "9fedb123-f210-4a91-b368-c90ec236c41a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Dataset_cross\n",
            "/content/drive/MyDrive/Dataset_cross/Test\n",
            "/content/drive/MyDrive/Dataset_cross/Test/normal\n",
            "/content/drive/MyDrive/Dataset_cross/Test/glaucoma\n",
            "/content/drive/MyDrive/Dataset_cross/Test/cataract\n",
            "/content/drive/MyDrive/Dataset_cross/Test/diabetic_retinopathy\n",
            "/content/drive/MyDrive/Dataset_cross/Train\n",
            "/content/drive/MyDrive/Dataset_cross/Train/cataract\n",
            "/content/drive/MyDrive/Dataset_cross/Train/normal\n",
            "/content/drive/MyDrive/Dataset_cross/Train/glaucoma\n",
            "/content/drive/MyDrive/Dataset_cross/Train/diabetic_retinopathy\n",
            "/content/drive/MyDrive/Dataset_cross/Pred\n"
          ]
        }
      ],
      "source": [
        "for dirname, _, filenames in os.walk('/content/drive/MyDrive/Dataset_cross'):\n",
        "    print(dirname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIIeIvAzDva7"
      },
      "source": [
        "# Define classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enLdk9PLDva7"
      },
      "outputs": [],
      "source": [
        "CLASSES = {'normal': 0, 'cataract': 1, 'glaucoma': 2, 'diabetic_retinopathy': 3}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4HN0cQsDva8"
      },
      "source": [
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxtsRMiJDva8"
      },
      "source": [
        "# Get Train Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71fhaNlBDva8"
      },
      "outputs": [],
      "source": [
        "def shuffle_prune(df, BATCH_SIZE):\n",
        "    df = shuffle(df, random_state=42)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    df = df[ : df.shape[0] // BATCH_SIZE * BATCH_SIZE]\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9L_lx_XDva8",
        "outputId": "fdda4e5b-8f4d-4ea7-ed31-44b5148f73cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train sample:  3328 {3: 864, 0: 850, 1: 820, 2: 794}\n"
          ]
        }
      ],
      "source": [
        "filenames = tf.io.gfile.glob('/content/drive/MyDrive/Dataset_cross/Train/*/*')\n",
        "image_path_df_train = pd.DataFrame(data={'filename': filenames, 'class': [x.split('/')[-2] for x in filenames]})\n",
        "image_path_df_train = shuffle_prune(image_path_df_train, BATCH_SIZE)\n",
        "image_path_df_train['class'] = image_path_df_train['class'].map(CLASSES)\n",
        "\n",
        "print('Train sample: ', len(image_path_df_train['class']), dict(image_path_df_train['class'].value_counts()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7g9n168Dva9"
      },
      "source": [
        "# Get Test Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9R8EmW5Dva-",
        "outputId": "6359aace-e2ba-4cb4-9e9c-83bd02f37cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test sample:  841 {'diabetic_retinopathy': 218, 'normal': 214, 'cataract': 207, 'glaucoma': 202}\n"
          ]
        }
      ],
      "source": [
        "filenames = tf.io.gfile.glob('/content/drive/MyDrive/Dataset_cross/Test/*/*')\n",
        "image_path_df_test = pd.DataFrame(data={'filename': filenames, 'class': [x.split('/')[-2] for x in filenames]})\n",
        "\n",
        "print('Test sample: ', len(image_path_df_test['class']), dict(image_path_df_test['class'].value_counts()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R3a6tmTDva9"
      },
      "source": [
        "# Get Validation sample from test sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g919_81Dva9",
        "outputId": "484a5671-ec57-49ff-c4a3-c783178a864f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test sample:  384 {3: 101, 2: 97, 1: 94, 0: 92}\n",
            "Val  sample:  384 {3: 100, 0: 95, 1: 95, 2: 94}\n"
          ]
        }
      ],
      "source": [
        "image_path_df_test, image_path_df_val  = train_test_split(image_path_df_test, test_size=0.5, random_state=42, stratify=image_path_df_test['class'])\n",
        "image_path_df_test = shuffle_prune(image_path_df_test, BATCH_SIZE)\n",
        "image_path_df_test['class'] = image_path_df_test['class'].map(CLASSES)\n",
        "\n",
        "image_path_df_val = shuffle_prune(image_path_df_val, BATCH_SIZE)\n",
        "image_path_df_val['class'] = image_path_df_val['class'].map(CLASSES)\n",
        "\n",
        "print('Test sample: ', len(image_path_df_test['class']), dict(image_path_df_test['class'].value_counts()))\n",
        "print('Val  sample: ', len(image_path_df_val['class']), dict(image_path_df_val['class'].value_counts()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR-RwruyDva-"
      },
      "source": [
        "# Get files for prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRQuxzKDDva9",
        "outputId": "8edfd055-cd61-4a0d-9f45-9348c23b02ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number filenames: 1008\n"
          ]
        }
      ],
      "source": [
        "filenames = tf.io.gfile.glob('/content/drive/MyDrive/Dataset_cross/Pred/*')\n",
        "\n",
        "image_path_df_predict = pd.DataFrame(data={'filename': filenames, 'class': np.nan})\n",
        "print(f'Number filenames: {len(image_path_df_predict)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP3IRJgTDva-"
      },
      "source": [
        "# Get arrays and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKxD4foZDva_"
      },
      "outputs": [],
      "source": [
        "def get_images_and_labels_arrays(df):\n",
        "    images = []\n",
        "    for file in df['filename']:\n",
        "        image = cv2.imread(file)\n",
        "        image = cv2.resize(image,IMAGE_SIZE)\n",
        "        images.append(image)\n",
        "    images = np.array(images)\n",
        "    \n",
        "    labels = df.loc[:, 'class']\n",
        "    return images, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sBdQIv2rDva_",
        "outputId": "5a96c7a1-9843-40f8-bf84-77b3ecdcc5df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of train set: (3328, 224, 224, 3)\n",
            "Shape of train set: (3328,)\n"
          ]
        }
      ],
      "source": [
        "train_images, train_labels = get_images_and_labels_arrays(image_path_df_train)\n",
        "\n",
        "print(f'Shape of train set: {train_images.shape}')\n",
        "print(f'Shape of train set: {train_labels.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtDgS9xvDva_"
      },
      "outputs": [],
      "source": [
        "val_images, val_labels = get_images_and_labels_arrays(image_path_df_val)\n",
        "\n",
        "print(f'Shape of validation set: {val_images.shape}')\n",
        "print(f'Shape of validation set: {val_labels.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4UqulS1DvbA"
      },
      "outputs": [],
      "source": [
        "test_images, test_labels = get_images_and_labels_arrays(image_path_df_test)\n",
        "\n",
        "print(f'Shape of test set: {test_images.shape}')\n",
        "print(f'Shape of test set: {test_labels.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOfQNBK6DvbA"
      },
      "source": [
        "# We have 4 classes in this work like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3eRvo2tDvbA"
      },
      "outputs": [],
      "source": [
        "f,ax = plt.subplots(3,3) \n",
        "f.subplots_adjust(0,0,3,3)\n",
        "for i in range(0,3,1):\n",
        "    for j in range(0,3,1):\n",
        "        rnd_number = randint(0,len(train_images))\n",
        "        ax[i,j].imshow(train_images[rnd_number])\n",
        "        ax[i,j].set_title([key for key, val in CLASSES.items() if val == train_labels[rnd_number]][0])\n",
        "        ax[i,j].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M230KiCvDvbA"
      },
      "source": [
        "# Define CNN Keras model and compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLu8ua8NDvbB"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    \n",
        "    with tf.device('/gpu:0'):\n",
        "    \n",
        "        input_layer = layers.Input(shape=(*IMAGE_SIZE, 3), name='input') \n",
        "        x = layers.BatchNormalization()(input_layer)\n",
        "\n",
        "        x = layers.Conv2D(filters=64, kernel_size=3, \n",
        "                          activation='relu', padding='same', \n",
        "                          name='conv2d_1')(x)\n",
        "        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_1')(x)\n",
        "        x = layers.Dropout(0.1, name='dropout_1')(x)\n",
        "\n",
        "        x = layers.Conv2D(filters=128, kernel_size=3, \n",
        "                          activation='relu', padding='same', \n",
        "                          name='conv2d_2')(x)\n",
        "        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_2')(x)\n",
        "        x = layers.Dropout(0.1, name='dropout_2')(x)\n",
        "\n",
        "        x = layers.Conv2D(filters=256, kernel_size=3, \n",
        "                          activation='relu', padding='same', \n",
        "                          name='conv2d_3')(x)\n",
        "        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_3')(x)\n",
        "        x = layers.Dropout(0.1, name='dropout_3')(x)\n",
        "\n",
        "        x = layers.Conv2D(filters=512, kernel_size=3, \n",
        "                          activation='relu', padding='same', \n",
        "                          name='conv2d_4')(x)\n",
        "        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_4')(x)\n",
        "        x = layers.Dropout(0.1, name='dropout_4')(x)\n",
        "\n",
        "        x = layers.Conv2D(filters=1024, kernel_size=3, \n",
        "                          activation='relu', padding='same', \n",
        "                          name='conv2d_5')(x)\n",
        "        x = layers.MaxPool2D(pool_size=2, name='maxpool2d_5')(x)\n",
        "        x = layers.Dropout(0.1, name='dropout_5')(x)\n",
        "        \n",
        "\n",
        "        x = layers.GlobalAveragePooling2D(name='global_average_pooling2d')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "       \n",
        "        x = layers.Dense(128,activation='relu')(x)\n",
        "        \n",
        "        output = layers.Dense(units=len(CLASSES), \n",
        "                              activation='softmax', \n",
        "                              name='output')(x)\n",
        "\n",
        "\n",
        "        model = Model (input_layer, output)    \n",
        "        model.compile(optimizer='adam', \n",
        "                      loss='sparse_categorical_crossentropy', \n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GSuw5vIDvbB"
      },
      "source": [
        "# Run model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQQEgYsbDvbB"
      },
      "outputs": [],
      "source": [
        "init_time = datetime.datetime.now()\n",
        "#learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 1, verbose=1, factor=0.3, min_lr=0.000001)\n",
        "early_stop = EarlyStopping(monitor = 'val_accuracy', mode='min', verbose = 1, patience = 5)\n",
        "trained = model.fit(\n",
        "                    train_images, train_labels,\n",
        "                    validation_data = (val_images, val_labels),\n",
        "                    batch_size = BATCH_SIZE, \n",
        "                    epochs=EPOCHS,\n",
        "                    #callbacks=[early_stop],\n",
        "                    shuffle = True\n",
        "    )\n",
        "\n",
        "requared_time = datetime.datetime.now() - init_time\n",
        "print(f'\\nRequired time:  {str(requared_time)}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBHkPKQFDvbC"
      },
      "source": [
        "# Training process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czgNkGJxDvbC"
      },
      "outputs": [],
      "source": [
        "plt.plot(trained.history['accuracy'])\n",
        "plt.plot(trained.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(trained.history['loss'])\n",
        "plt.plot(trained.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNFfVG5UDvbC"
      },
      "source": [
        "# Evaluate the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRpsjbJaDvbC"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print('accuracy:', test_acc, '  loss: ',test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIIhdnsEDvbD"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR5v_viCDvbD"
      },
      "outputs": [],
      "source": [
        "predict = np.argmax(model.predict(test_images), axis=1)\n",
        "predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IJkCkeKDvbD"
      },
      "source": [
        "# Classification report & confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hxtt7MIfDvbE"
      },
      "outputs": [],
      "source": [
        "print(classification_report(test_labels, predict), '\\n')\n",
        "cm = confusion_matrix(test_labels, predict)\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.0f', cbar=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr1RQMzBDvbE"
      },
      "source": [
        "# Let's try pretrained model for example VGG19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDkM4QTJDvbE"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    with tf.device('/gpu:0'):\n",
        "        pretrained_model = tf.keras.applications.VGG19(\n",
        "            weights='imagenet',\n",
        "            include_top=False ,\n",
        "            input_shape=[*IMAGE_SIZE, 3]\n",
        "        )\n",
        "        pretrained_model.trainable = False\n",
        "\n",
        "        \n",
        "        \n",
        "        input_layer = layers.Input(shape=(*IMAGE_SIZE, 3), name='input') \n",
        "        \n",
        "        x = pretrained_model(input_layer)\n",
        "\n",
        "        x = layers.GlobalAveragePooling2D(name='global_average_pooling2d')(x)\n",
        "        x = layers.BatchNormalization()(x)       \n",
        "        x = layers.Dense(128,activation='relu')(x)\n",
        "\n",
        "        \n",
        "        output = layers.Dense(units=len(CLASSES), \n",
        "                              activation='softmax', \n",
        "                              name='output')(x)\n",
        "\n",
        "\n",
        "        model = Model (input_layer, output)    \n",
        "        model.compile(optimizer='adam', \n",
        "                      loss='sparse_categorical_crossentropy', \n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL8pU4S3DvbE"
      },
      "outputs": [],
      "source": [
        "init_time = datetime.datetime.now()\n",
        "#learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 1, verbose=1, factor=0.3, min_lr=0.000001)\n",
        "early_stop = EarlyStopping(monitor = 'val_accuracy', mode='min', verbose = 1, patience = 5)\n",
        "trained = model.fit(\n",
        "                    train_images, train_labels,\n",
        "                    validation_data = (val_images, val_labels),\n",
        "                    batch_size = BATCH_SIZE, \n",
        "                    epochs=EPOCHS,\n",
        "                    #callbacks=[early_stop],\n",
        "                    shuffle = True\n",
        "\n",
        "    )\n",
        "\n",
        "requared_time = datetime.datetime.now() - init_time\n",
        "print(f'\\nRequired time:  {str(requared_time)}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjYllOsHDvbE"
      },
      "outputs": [],
      "source": [
        "plt.plot(trained.history['accuracy'])\n",
        "plt.plot(trained.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(trained.history['loss'])\n",
        "plt.plot(trained.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lyt0KuYnDvbE"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print('\\naccuracy:', test_acc, '  loss: ',test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD31CX4-DvbF"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5oknFkhDvbF"
      },
      "outputs": [],
      "source": [
        "predict = np.argmax(model.predict(test_images), axis=1)\n",
        "predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvU9BrCRDvbF"
      },
      "source": [
        "# Classification report & confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQaFHG0aDvbF"
      },
      "outputs": [],
      "source": [
        "print(classification_report(test_labels, predict), '\\n')\n",
        "cm = confusion_matrix(test_labels, predict)\n",
        "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt='.0f', cbar=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBSZM2FzDvbF"
      },
      "source": [
        "# Let's predict unlabeled images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyCqmJinDvbG"
      },
      "outputs": [],
      "source": [
        "to_predict_images, to_predict_labels = get_images_and_labels_arrays(image_path_df_predict)\n",
        "print(f'Shape of images set to prediction: {to_predict_images.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRjrGOeBDvbG"
      },
      "outputs": [],
      "source": [
        "predict = np.argmax(model.predict(to_predict_images), axis=1)\n",
        "predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs8PXhmMDvbG"
      },
      "source": [
        "# Let's check random images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq73tLsXDvbG"
      },
      "outputs": [],
      "source": [
        "f,ax = plt.subplots(5,5) \n",
        "f.subplots_adjust(0,0,3,3)\n",
        "for i in range(0,5,1):\n",
        "    for j in range(0,5,1):\n",
        "        rnd_number = randint(0,len(predict))\n",
        "        ax[i,j].imshow(to_predict_images[rnd_number])\n",
        "        ax[i,j].set_title([key for key, val in CLASSES.items() if val == predict[rnd_number]][0])\n",
        "        ax[i,j].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjlZ_a81u05I"
      },
      "source": [
        "### **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4_D1AeDn40m"
      },
      "outputs": [],
      "source": [
        "filenames = tf.io.gfile.glob('/content/drive/MyDrive/singleInput/*')\n",
        "\n",
        "image_path_df_predict = pd.DataFrame(data={'filename': filenames, 'class': np.nan})\n",
        "print(f'Number filenames: {len(image_path_df_predict)}')\n",
        "print(filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sez59FppyL1"
      },
      "outputs": [],
      "source": [
        "to_predict_images, to_predict_labels = get_images_and_labels_arrays(image_path_df_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF_kN-7wqAg8"
      },
      "outputs": [],
      "source": [
        "predict = np.argmax(model.predict(to_predict_images), axis=1)\n",
        "predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOIMx-unqFmF"
      },
      "outputs": [],
      "source": [
        "f,ax = plt.subplots(len(predict)) \n",
        "f.subplots_adjust(0,0,3,3)\n",
        "i = 0\n",
        "for j in range(0,len(predict),1):\n",
        "    ax[j].imshow(to_predict_images[j])\n",
        "    ax[j].set_title([key for key, val in CLASSES.items() if val == predict[j]][0])\n",
        "    ax[j].axis('off')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}